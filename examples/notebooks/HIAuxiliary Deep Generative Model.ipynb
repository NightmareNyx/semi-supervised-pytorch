{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "sys.path.append(\"../../semi-supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import batch_normalization, dynamic_partition, dynamic_stitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIAuxiliary Deep Generative Model\n",
    "\n",
    "The Auxiliary Deep Generative Model [[Maal√∏e, 2016]](https://arxiv.org/abs/1602.05473) posits a model that with an auxiliary latent variable $a$ that infers the variables $z$ and $y$. This helps in terms of semi-supervised learning by delegating causality to their respective variables. This model was state-of-the-art in semi-supervised until 2017, and is still very powerful with an MNIST accuracy of *99.4%* using just 10 labelled examples per class.\n",
    "\n",
    "<img src=\"../images/adgm.png\" width=\"400px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The lower bound we derived in the notebook for the **deep generative model** is similar to the one for the ADGM. Here, we also need to integrate over a continuous auxiliary variable $a$.\n",
    "\n",
    "For labelled data, the lower bound is given by.\n",
    "\\begin{align}\n",
    "\\log p(x,y) &= \\log \\int \\int p(x, y, a, z) \\ dz \\ da\\\\\n",
    "&\\geq \\mathbb{E}_{q(a,z|x,y)} \\bigg [\\log \\frac{p(x,y,a,z)}{q(a,z|x,y)} \\bigg ] = - \\mathcal{L}(x,y)\n",
    "\\end{align}\n",
    "\n",
    "Again when no label information is available we sum out all of the labels.\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(x) &= \\log \\int \\sum_{y} \\int p(x, y, a, z) \\ dz \\ da\\\\\n",
    "&\\geq \\mathbb{E}_{q(a,y,z|x)} \\bigg [\\log \\frac{p(x,y,a,z)}{q(a,y,z |x)} \\bigg ] = - \\mathcal{U}(x)\n",
    "\\end{align}\n",
    "\n",
    "Where we decompose the q-distribution into its constituent parts. $q(a, y, z|x) = q(z|a,y,x)q(y|a,x)q(a|x)$, which is also what can be seen in the figure.\n",
    "\n",
    "The distribution over $a$ is similar to $z$ in the sense that it is also a diagonal Gaussian distribution. However by introducing the auxiliary variable we allow for $z$ to become arbitrarily complex - something we can also see when using normalizing flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datautils import get_mnist\n",
    "\n",
    "# # Only use 10 labelled examples per class\n",
    "# # The rest of the data is unlabelled.\n",
    "# labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=64, labels_per_class=10)\n",
    "# alpha = 0.1 * (len(unlabelled) + len(labelled)) / len(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_functions as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'defaultCredit'\n",
    "data_file = dataset+'/data.csv'\n",
    "types_file = dataset+'/data_types.csv'\n",
    "miss_file = dataset+'/Missing10-50_1.csv'\n",
    "true_miss_file = None\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, types_dict, miss_mask, true_miss_mask, n_samples = rf.read_data(data_file, types_file,\n",
    "                                                                                        miss_file,\n",
    "                                                                                        true_miss_file)\n",
    "# Randomize the data in the mini-batches\n",
    "random_perm = np.random.permutation(range(np.shape(train_data)[0]))\n",
    "train_data = train_data[random_perm, :]\n",
    "miss_mask = miss_mask[random_perm, :]\n",
    "true_miss_mask = true_miss_mask[random_perm, :]\n",
    "\n",
    "# Check batch size\n",
    "if batch_size > n_samples:\n",
    "    batch_size = n_samples\n",
    "    \n",
    "for t in types_dict:\n",
    "    t['dim'] = int(t['dim'])\n",
    "\n",
    "# Compute the real miss_mask\n",
    "miss_mask = np.multiply(miss_mask, true_miss_mask)\n",
    "\n",
    "train_data = torch.Tensor(train_data)\n",
    "miss_mask = torch.Tensor(miss_mask)\n",
    "\n",
    "labelled = train_data[miss_mask[:,-1] == 1, :-2], train_data[miss_mask[:,-1] == 1, -2:]\n",
    "unlabelled = train_data[miss_mask[:,-1] == 0, :-2], train_data[miss_mask[:,-1] == 0, -2:]\n",
    "miss_mask = miss_mask[:,:-1]\n",
    "type_label = types_dict[-1]\n",
    "types_dict = types_dict[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 * (len(unlabelled) + len(labelled)) / len(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = labelled\n",
    "u, v = unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get an integer number of batches\n",
    "n_batches = int(np.floor(max(x.size()[0], u.size()[0]) / batch_size))\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x.size()[0] < u.size()[0]:\n",
    "    xx = torch.zeros_like(u)\n",
    "    yy = torch.zeros_like(v)\n",
    "    xx[:x.size()[0], :] = x\n",
    "    xx[x.size()[0]:, :] = x[:u.size()[0] - x.size()[0], :]\n",
    "    x = xx\n",
    "    yy[:y.size()[0], :] = y\n",
    "    yy[y.size()[0]:, :] = y[:v.size()[0] - y.size()[0], :]\n",
    "    y = yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15000, 94]),\n",
       " torch.Size([15000, 94]),\n",
       " torch.Size([15000, 2]),\n",
       " torch.Size([15000, 2]),\n",
       " 23)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.size(), x.size(), y.size(), v.size(), len(types_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e2c0960fe70c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHIAuxiliaryDeepGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/models/dgm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims, types_list)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIAuxiliaryDeepGenerativeModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# everything else the same # TODO ???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHIDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_dim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes_list\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# p(x|z,g(z))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/models/vae.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims, types_list)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# deterministic homogeneous gamma layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_input_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_dim_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obs_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;31m# self.reconstruction = nn.Linear(h_dim[-1], x_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# self.output_activation = nn.Sigmoid()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/models/vae.py\u001b[0m in \u001b[0;36mget_obs_layers\u001b[0;34m(self, gamma_dim)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mobs_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mobs_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_dim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# log pi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ord'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 obs_layers.append(nn.ModuleList[nn.Linear(gamma_dim, type_dim - 1),  # theta\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from models import HIAuxiliaryDeepGenerativeModel\n",
    "\n",
    "y_dim = y.size()[1]\n",
    "z_dim = 5\n",
    "a_dim = 5 \n",
    "gamma_dim = 5\n",
    "h_dim = [5]\n",
    "input_dim = x.size()[1]\n",
    "\n",
    "model = HIAuxiliaryDeepGenerativeModel([input_dim, y_dim, z_dim, a_dim, gamma_dim, h_dim], types_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15000, 94])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'pos', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'cat', 'dim': 3, 'nclass': '3'},\n",
       " {'type': 'cat', 'dim': 7, 'nclass': '7'},\n",
       " {'type': 'cat', 'dim': 4, 'nclass': '4'},\n",
       " {'type': 'count', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'ordinal', 'dim': 11, 'nclass': '11'},\n",
       " {'type': 'ordinal', 'dim': 11, 'nclass': '11'},\n",
       " {'type': 'ordinal', 'dim': 11, 'nclass': '11'},\n",
       " {'type': 'ordinal', 'dim': 11, 'nclass': '11'},\n",
       " {'type': 'ordinal', 'dim': 11, 'nclass': '11'},\n",
       " {'type': 'ordinal', 'dim': 11, 'nclass': '11'},\n",
       " {'type': 'real', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'real', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'real', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'real', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'real', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'real', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'pos', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'pos', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'pos', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'pos', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'pos', 'dim': 1, 'nclass': ''},\n",
       " {'type': 'pos', 'dim': 1, 'nclass': ''}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(r, x):\n",
    "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from inference import HISVI, DeterministicWarmup\n",
    "\n",
    "# We will need to use warm-up in order to achieve good performance.\n",
    "# Over 200 calls to SVI we change the autoencoder from\n",
    "# deterministic to stochastic.\n",
    "beta = DeterministicWarmup(n=200)\n",
    "\n",
    "\n",
    "if cuda: model = model.cuda()\n",
    "elbo = HISVI(model, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in tqdm(range(100)):\n",
    "    model.train()\n",
    "    total_loss, L_loss, U_loss, class_loss, accuracy = (0, 0, 0, 0, 0)\n",
    "    for i in range(n_batches):\n",
    "        # Create inputs for the feed_dict\n",
    "        data_tensor, labels, batch_miss_mask = rf.next_batch(x, y, types_dict, miss_mask,\n",
    "                                                         batch_size, index_batch=i)\n",
    "        unl_data_tensor, hid_labels, unl_batch_miss_mask = rf.next_batch(u, v, types_dict, miss_mask,\n",
    "                                                         batch_size, index_batch=i)\n",
    "\n",
    "        # Delete not known data (input zeros)\n",
    "        data_observed = data_tensor * batch_miss_mask\n",
    "        unl_data_observed = unl_data_tensor * unl_batch_miss_mask\n",
    "\n",
    "        # easier names\n",
    "        x_batch = data_observed\n",
    "        u_batch = unl_data_observed\n",
    "        y_batch = labels\n",
    "        v_batch = hid_labels\n",
    "\n",
    "        ######\n",
    "        if cuda:\n",
    "            # They need to be on the same device and be synchronized.\n",
    "            x_batch, y_batch = x_batch.cuda(device=0), y_batch.cuda(device=0)\n",
    "            u_batch, v_batch = u_batch.cuda(device=0), v_batch.cuda(device=0)\n",
    "\n",
    "        L = -elbo(x_batch, y_batch, batch_miss_mask)\n",
    "        \n",
    "        # Add auxiliary classification loss q(y|x,a)\n",
    "        logits = model.classify(x_batch)\n",
    "        \n",
    "        U = -elbo(u_batch, y=None, miss_list=unl_batch_miss_mask)\n",
    "\n",
    "        # Regular cross entropy\n",
    "        classication_loss = torch.sum(y_batch * torch.log(logits + 1e-8), dim=1).mean()\n",
    "        J_alpha = L - alpha * classication_loss + U\n",
    "#         J_alpha = L\n",
    "\n",
    "        J_alpha.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += J_alpha.item()\n",
    "        U_loss += U.item()\n",
    "        L_loss += L.item()\n",
    "        class_loss += classication_loss.item()\n",
    "        accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y_batch, 1)[1].data).float()).item() / n_batches\n",
    "        ######\n",
    "    samples_z = model.samples_z\n",
    "    samples_qa = model.samples_qa\n",
    "    samples_pa = model.samples_pa\n",
    "    model.samples_z = []\n",
    "    model.samples_qa = []\n",
    "    model.samples_pa = []\n",
    "    print('Epoch:', epoch+1)\n",
    "    print('Loss:', total_loss)\n",
    "    print('---- U:', U_loss)\n",
    "    print('---- L:', L_loss)\n",
    "    print('---- cl/tion loss:', class_loss)\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('-------------------------------------------\\n')\n",
    "    losses.append((total_loss, U_loss, L_loss, class_loss, accuracy))\n",
    "        \n",
    "         # TODO\n",
    "#     if epoch % 1 == 0:\n",
    "#         model.eval()\n",
    "#         m = len(unlabelled)\n",
    "#         print(\"Epoch: {}\".format(epoch))\n",
    "#         print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
    "\n",
    "#         total_loss, accuracy = (0, 0)\n",
    "#         for x, y in validation:\n",
    "#             x, y = Variable(x), Variable(y)\n",
    "\n",
    "#             if cuda:\n",
    "#                 x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "#             L = -elbo(x, y)\n",
    "#             U = -elbo(x)\n",
    "\n",
    "#             logits = model.classify(x)\n",
    "#             classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "#             J_alpha = L + alpha * classication_loss + U\n",
    "\n",
    "#             total_loss += J_alpha.data[0]\n",
    "\n",
    "#             _, pred_idx = torch.max(logits, 1)\n",
    "#             _, lab_idx = torch.max(y, 1)\n",
    "#             accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "#         m = len(validation)\n",
    "#         print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "for l, title in zip(losses.T, ['Total loss', \"Unlabeled loss\", 'Labeled loss', 'Classification loss', 'accuracy']):\n",
    "    plt.plot(l)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot a\n",
    "# try without classloss \n",
    "# try breast\n",
    "# try more dimensions\n",
    "# try different Œ± because classloss is very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = torch.cat([z for z, _ in samples_z], dim=0)\n",
    "ZY = torch.cat([y for _, y in samples_z], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "ZZ_pca = pca.fit_transform(ZZ.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ZZ_pca[:,0], ZZ_pca[:,1], c=np.argmax(ZY, axis=1), alpha=0.5, s=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=250)\n",
    "ZZ_tsne = tsne.fit_transform(ZZ.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ZZ_tsne[:,0], ZZ_tsne[:,1], c=np.argmax(ZY, axis=1), alpha=0.5, s=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zz, yy in samples_z:\n",
    "    zz = zz.detach().numpy()\n",
    "    plt.scatter(zz[:,0], zz[:,1], c=np.argmax(yy, axis=1), alpha=0.5, s=4)\n",
    "plt.title('Latent Space for z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa, yy in samples_qa:\n",
    "    qa = qa.detach().numpy()\n",
    "    plt.scatter(qa[:,0], qa[:,1], c=np.argmax(yy, axis=1), alpha=0.5, s=4)\n",
    "plt.title('Latent Space for a')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pa, yy in samples_pa:\n",
    "    pa = pa.detach().numpy()\n",
    "    plt.scatter(pa[:,0], pa[:,1], c=np.argmax(yy, axis=1), alpha=0.5, s=4)\n",
    "plt.title('Latent Space for a')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library is conventially packed with the `SVI` method that does all of the work of calculating the lower bound for both labelled and unlabelled data depending on whether the label is given. It also manages to perform the enumeration of all the labels.\n",
    "\n",
    "Remember that the labels have to be in a *one-hot encoded* format in order to work with SVI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional generation\n",
    "\n",
    "When the model is done training you can generate samples conditionally given some normal distributed noise $z$ and a label $y$.\n",
    "\n",
    "*The model below has only trained for 10 iterations, so the perfomance is not representative*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import onehot\n",
    "model.eval()\n",
    "\n",
    "z = Variable(torch.randn(16, 32))\n",
    "\n",
    "# Generate a batch of 5s\n",
    "y = Variable(onehot(10)(5).repeat(16, 1))\n",
    "\n",
    "x_mu = model.sample(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 16, figsize=(18, 12))\n",
    "\n",
    "samples = x_mu.data.view(-1, 28, 28).numpy()\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(samples[i])\n",
    "    ax.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
