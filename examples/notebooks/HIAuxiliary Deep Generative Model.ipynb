{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../semi-supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import batch_normalization, dynamic_partition, dynamic_stitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIAuxiliary Deep Generative Model\n",
    "\n",
    "The Auxiliary Deep Generative Model [[Maal√∏e, 2016]](https://arxiv.org/abs/1602.05473) posits a model that with an auxiliary latent variable $a$ that infers the variables $z$ and $y$. This helps in terms of semi-supervised learning by delegating causality to their respective variables. This model was state-of-the-art in semi-supervised until 2017, and is still very powerful with an MNIST accuracy of *99.4%* using just 10 labelled examples per class.\n",
    "\n",
    "<img src=\"../images/adgm.png\" width=\"400px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The lower bound we derived in the notebook for the **deep generative model** is similar to the one for the ADGM. Here, we also need to integrate over a continuous auxiliary variable $a$.\n",
    "\n",
    "For labelled data, the lower bound is given by.\n",
    "\\begin{align}\n",
    "\\log p(x,y) &= \\log \\int \\int p(x, y, a, z) \\ dz \\ da\\\\\n",
    "&\\geq \\mathbb{E}_{q(a,z|x,y)} \\bigg [\\log \\frac{p(x,y,a,z)}{q(a,z|x,y)} \\bigg ] = - \\mathcal{L}(x,y)\n",
    "\\end{align}\n",
    "\n",
    "Again when no label information is available we sum out all of the labels.\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(x) &= \\log \\int \\sum_{y} \\int p(x, y, a, z) \\ dz \\ da\\\\\n",
    "&\\geq \\mathbb{E}_{q(a,y,z|x)} \\bigg [\\log \\frac{p(x,y,a,z)}{q(a,y,z |x)} \\bigg ] = - \\mathcal{U}(x)\n",
    "\\end{align}\n",
    "\n",
    "Where we decompose the q-distribution into its constituent parts. $q(a, y, z|x) = q(z|a,y,x)q(y|a,x)q(a|x)$, which is also what can be seen in the figure.\n",
    "\n",
    "The distribution over $a$ is similar to $z$ in the sense that it is also a diagonal Gaussian distribution. However by introducing the auxiliary variable we allow for $z$ to become arbitrarily complex - something we can also see when using normalizing flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datautils import get_mnist\n",
    "\n",
    "# # Only use 10 labelled examples per class\n",
    "# # The rest of the data is unlabelled.\n",
    "# labelled, unlabelled, validation = get_mnist(location=\"./\", batch_size=64, labels_per_class=10)\n",
    "# alpha = 0.1 * (len(unlabelled) + len(labelled)) / len(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_functions as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'Wine/data.csv'\n",
    "types_file = 'Wine/data_types.csv'\n",
    "miss_file = 'Wine/Missing10-50_1.csv'\n",
    "true_miss_file = None\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, types_dict, miss_mask, true_miss_mask, n_samples = rf.read_data(data_file, types_file,\n",
    "                                                                                        miss_file,\n",
    "                                                                                        true_miss_file)\n",
    "# Randomize the data in the mini-batches\n",
    "random_perm = np.random.permutation(range(np.shape(train_data)[0]))\n",
    "train_data_aux = train_data[random_perm, :]\n",
    "miss_mask_aux = miss_mask[random_perm, :]\n",
    "true_miss_mask_aux = true_miss_mask[random_perm, :]\n",
    "\n",
    "# Check batch size\n",
    "if batch_size > n_samples:\n",
    "    batch_size = n_samples\n",
    "    \n",
    "for t in types_dict:\n",
    "    t['dim'] = int(t['dim'])\n",
    "\n",
    "# Compute the real miss_mask\n",
    "miss_mask = np.multiply(miss_mask, true_miss_mask)\n",
    "\n",
    "train_data = torch.Tensor(train_data)\n",
    "miss_mask = torch.Tensor(miss_mask)\n",
    "\n",
    "labelled = train_data[miss_mask[:,-1] == 0, :-2], train_data[miss_mask[:,-1] == 0, -2:]\n",
    "unlabelled = train_data[miss_mask[:,-1] == 1, :-2], train_data[miss_mask[:,-1] == 1, -2:]\n",
    "miss_mask = miss_mask[:,:-1]\n",
    "type_label = types_dict[-1]\n",
    "types_dict = types_dict[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 * (len(unlabelled) + len(labelled)) / len(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = labelled\n",
    "u, v = unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get an integer number of batches\n",
    "n_batches = int(np.floor(max(x.size()[0], u.size()[0]) / batch_size))\n",
    "n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x.size()[0] < u.size()[0]:\n",
    "    xx = torch.zeros_like(u)\n",
    "    yy = torch.zeros_like(v)\n",
    "    xx[:x.size()[0], :] = x\n",
    "    xx[x.size()[0]:, :] = x[:u.size()[0] - x.size()[0], :]\n",
    "    x = xx\n",
    "    yy[:y.size()[0], :] = y\n",
    "    yy[y.size()[0]:, :] = y[:v.size()[0] - y.size()[0], :]\n",
    "    y = yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3249, 12]),\n",
       " torch.Size([3249, 12]),\n",
       " torch.Size([3249, 2]),\n",
       " torch.Size([3249, 2]),\n",
       " 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.size(), x.size(), y.size(), v.size(), len(types_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HIAuxiliaryDeepGenerativeModel(\n",
       "  (encoder): Encoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=16, out_features=5, bias=True)\n",
       "    )\n",
       "    (sample): GaussianSample(\n",
       "      (mu): Linear(in_features=5, out_features=2, bias=True)\n",
       "      (log_var): Linear(in_features=5, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): HIDecoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=4, out_features=5, bias=True)\n",
       "    )\n",
       "    (gamma_layer): Linear(in_features=5, out_features=60, bias=True)\n",
       "    (obs_layer): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (6): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (7): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (8): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (9): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (10): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "        (1): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "      (11): ModuleList(\n",
       "        (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (dense): Linear(in_features=14, out_features=5, bias=True)\n",
       "    (logits): Linear(in_features=5, out_features=2, bias=True)\n",
       "  )\n",
       "  (aux_encoder): Encoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=12, out_features=5, bias=True)\n",
       "    )\n",
       "    (sample): GaussianSample(\n",
       "      (mu): Linear(in_features=5, out_features=2, bias=True)\n",
       "      (log_var): Linear(in_features=5, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (aux_decoder): Encoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=16, out_features=5, bias=True)\n",
       "    )\n",
       "    (sample): GaussianSample(\n",
       "      (mu): Linear(in_features=5, out_features=2, bias=True)\n",
       "      (log_var): Linear(in_features=5, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import HIAuxiliaryDeepGenerativeModel\n",
    "\n",
    "y_dim = y.size()[1]\n",
    "z_dim = 2\n",
    "a_dim = 2\n",
    "gamma_dim = 5\n",
    "h_dim = [5]\n",
    "input_dim = x.size()[1]\n",
    "\n",
    "model = HIAuxiliaryDeepGenerativeModel([input_dim, y_dim, z_dim, a_dim, gamma_dim, h_dim], types_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(r, x):\n",
    "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from inference import HISVI, DeterministicWarmup\n",
    "\n",
    "# We will need to use warm-up in order to achieve good performance.\n",
    "# Over 200 calls to SVI we change the autoencoder from\n",
    "# deterministic to stochastic.\n",
    "beta = DeterministicWarmup(n=200)\n",
    "\n",
    "\n",
    "if cuda: model = model.cuda()\n",
    "elbo = HISVI(model, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n",
      "tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<NegBackward>) tensor(nan, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b9b41440a21b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_miss_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munl_batch_miss_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munl_normalization_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Add auxiliary classification loss q(y|x,a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/intel/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/inference/variational.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, miss_list, norm_params)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# samples of x from decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# and their parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mlog_p_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_p_x_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Eq[log_p(x|z, y)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/intel/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/models/dgm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, miss_list, norm_params)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Generative p(x|g(z),y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         log_p_x, log_p_x_missing, samples_x, params_x = self.decoder(torch.cat([z, y], dim=1), x, miss_list,\n\u001b[0;32m--> 209\u001b[0;31m                                                                      norm_params)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# Generative p(a|z,y,x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/intel/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/models/vae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, batch_x, miss_list, norm_params)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mgamma_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_estimation_from_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_grouped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         log_p_x, log_p_x_missing, samples_x, params_x = self.loglik_and_reconstruction(theta, batch_x,\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/models/vae.py\u001b[0m in \u001b[0;36mtheta_estimation_from_gamma\u001b[0;34m(self, gamma, miss_list)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mcondition_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamic_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiss_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m# Different layer models for each type of variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserved_data_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserved_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/models/vae.py\u001b[0m in \u001b[0;36mobserved_data_layer\u001b[0;34m(self, observed_data, missing_data, condition_indices, i)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mmiss_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_layer_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# Join back the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamic_stitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmiss_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/semi-supervised-pytorch/semi-supervised/utils.py\u001b[0m in \u001b[0;36mdynamic_stitch\u001b[0;34m(indices, data)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss, accuracy = (0, 0)\n",
    "    for i in range(n_batches):\n",
    "        # Create inputs for the feed_dict\n",
    "        data_tensor, labels, batch_miss_mask = rf.next_batch(x, y, types_dict, miss_mask,\n",
    "                                                         batch_size, index_batch=i)\n",
    "        unl_data_tensor, hid_labels, unl_batch_miss_mask = rf.next_batch(u, v, types_dict, miss_mask,\n",
    "                                                         batch_size, index_batch=i)\n",
    "\n",
    "        # Delete not known data (input zeros)\n",
    "        data_observed = data_tensor * batch_miss_mask\n",
    "        unl_data_observed = unl_data_tensor * unl_batch_miss_mask\n",
    "\n",
    "        # easier names\n",
    "        x_batch = data_observed\n",
    "        u_batch = unl_data_observed\n",
    "        y_batch = labels\n",
    "        v_batch = hid_labels\n",
    "\n",
    "        ######\n",
    "        if cuda:\n",
    "            # They need to be on the same device and be synchronized.\n",
    "            x_batch, y_batch = x_batch.cuda(device=0), y_batch.cuda(device=0)\n",
    "            u_batch, v_batch = u_batch.cuda(device=0), v_batch.cuda(device=0)\n",
    "\n",
    "        L = -elbo(x_batch, y_batch, batch_miss_mask)\n",
    "        \n",
    "        # Add auxiliary classification loss q(y|x,a)\n",
    "        logits = model.classify(x_batch)\n",
    "        \n",
    "        U = -elbo(u_batch, y=None, miss_list=unl_batch_miss_mask)\n",
    "\n",
    "        # Regular cross entropy\n",
    "        classication_loss = torch.sum(y_batch * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "        J_alpha = L - alpha * classication_loss + U\n",
    "        \n",
    "        print(J_alpha)\n",
    "\n",
    "        J_alpha.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += J_alpha.item()\n",
    "        accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y_batch, 1)[1].data).float())\n",
    "        ######\n",
    "        \n",
    "         # TODO\n",
    "#     if epoch % 1 == 0:\n",
    "#         model.eval()\n",
    "#         m = len(unlabelled)\n",
    "#         print(\"Epoch: {}\".format(epoch))\n",
    "#         print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
    "\n",
    "#         total_loss, accuracy = (0, 0)\n",
    "#         for x, y in validation:\n",
    "#             x, y = Variable(x), Variable(y)\n",
    "\n",
    "#             if cuda:\n",
    "#                 x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "#             L = -elbo(x, y)\n",
    "#             U = -elbo(x)\n",
    "\n",
    "#             logits = model.classify(x)\n",
    "#             classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "#             J_alpha = L + alpha * classication_loss + U\n",
    "\n",
    "#             total_loss += J_alpha.data[0]\n",
    "\n",
    "#             _, pred_idx = torch.max(logits, 1)\n",
    "#             _, lab_idx = torch.max(y, 1)\n",
    "#             accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "#         m = len(validation)\n",
    "#         print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library is conventially packed with the `SVI` method that does all of the work of calculating the lower bound for both labelled and unlabelled data depending on whether the label is given. It also manages to perform the enumeration of all the labels.\n",
    "\n",
    "Remember that the labels have to be in a *one-hot encoded* format in order to work with SVI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional generation\n",
    "\n",
    "When the model is done training you can generate samples conditionally given some normal distributed noise $z$ and a label $y$.\n",
    "\n",
    "*The model below has only trained for 10 iterations, so the perfomance is not representative*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import onehot\n",
    "model.eval()\n",
    "\n",
    "z = Variable(torch.randn(16, 32))\n",
    "\n",
    "# Generate a batch of 5s\n",
    "y = Variable(onehot(10)(5).repeat(16, 1))\n",
    "\n",
    "x_mu = model.sample(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 16, figsize=(18, 12))\n",
    "\n",
    "samples = x_mu.data.view(-1, 28, 28).numpy()\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(samples[i])\n",
    "    ax.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
